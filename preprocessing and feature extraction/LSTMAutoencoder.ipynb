{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import hour, when\n",
    "\n",
    "# Define Work Hours (8 AM - 6 PM)\n",
    "WORK_HOURS_START = 8\n",
    "WORK_HOURS_END = 18\n",
    "\n",
    "# Extract Hour from Timestamp and Categorize as Work or Off-Hours\n",
    "logon_df = logon_df.withColumn(\"hour\", hour(col(\"date\")))\n",
    "logon_df = logon_df.withColumn(\"work_hours\", when((col(\"hour\") >= WORK_HOURS_START) & (col(\"hour\") <= WORK_HOURS_END), \"work\").otherwise(\"offhours\"))\n",
    "\n",
    "device_df = device_df.withColumn(\"hour\", hour(col(\"date\")))\n",
    "device_df = device_df.withColumn(\"work_hours\", when((col(\"hour\") >= WORK_HOURS_START) & (col(\"hour\") <= WORK_HOURS_END), \"work\").otherwise(\"offhours\"))\n",
    "\n",
    "http_df = http_df.withColumn(\"hour\", hour(col(\"date\")))\n",
    "http_df = http_df.withColumn(\"work_hours\", when((col(\"hour\") >= WORK_HOURS_START) & (col(\"hour\") <= WORK_HOURS_END), \"work\").otherwise(\"offhours\"))\n",
    "\n",
    "email_df = email_df.withColumn(\"hour\", hour(col(\"date\")))\n",
    "email_df = email_df.withColumn(\"work_hours\", when((col(\"hour\") >= WORK_HOURS_START) & (col(\"hour\") <= WORK_HOURS_END), \"work\").otherwise(\"offhours\"))\n",
    "\n",
    "file_df = file_df.withColumn(\"hour\", hour(col(\"date\")))\n",
    "file_df = file_df.withColumn(\"work_hours\", when((col(\"hour\") >= WORK_HOURS_START) & (col(\"hour\") <= WORK_HOURS_END), \"work\").otherwise(\"offhours\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat, lit\n",
    "\n",
    "# Append Work/Off-Hours to Activity Names\n",
    "logon_df = logon_df.withColumn(\"activity\", concat(col(\"activity\"), lit(\"_\"), col(\"work_hours\")))\n",
    "device_df = device_df.withColumn(\"activity\", concat(col(\"activity\"), lit(\"_\"), col(\"work_hours\")))\n",
    "http_df = http_df.withColumn(\"activity\", concat(lit(\"http_access_\"), col(\"work_hours\")))  # URLs don't have predefined activity names\n",
    "email_df = email_df.withColumn(\"activity\", concat(lit(\"email_sent_\"), col(\"work_hours\")))  # Email content treated as sent activity\n",
    "file_df = file_df.withColumn(\"activity\", concat(lit(\"file_transfer_\"), col(\"work_hours\")))  # File access treated as transfer\n",
    "\n",
    "# Apply Activity Encoding AFTER Work/Off-Hours Classification\n",
    "logon_df = logon_df.withColumn(\"activity_encoded\", activity_mapping_udf(col(\"activity\")))\n",
    "device_df = device_df.withColumn(\"activity_encoded\", activity_mapping_udf(col(\"activity\")))\n",
    "http_df = http_df.withColumn(\"activity_encoded\", activity_mapping_udf(col(\"activity\")))\n",
    "email_df = email_df.withColumn(\"activity_encoded\", activity_mapping_udf(col(\"activity\")))\n",
    "file_df = file_df.withColumn(\"activity_encoded\", activity_mapping_udf(col(\"activity\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge All Processed DataFrames into User-Day Sequences\n",
    "user_day_sequences = (\n",
    "    logon_df.union(device_df).union(http_df).union(email_df).union(file_df)\n",
    "    .groupBy(\"user\", \"date\")\n",
    "    .agg(collect_list(\"activity_encoded\").alias(\"activity_sequence\"))\n",
    ")\n",
    "\n",
    "# Save Final Sequence Data for LSTM Training\n",
    "user_day_sequences.write.csv(\"lstm_input.csv\", header=True)\n",
    "\n",
    "print(\"✅ Successfully classified activities as work/off-hours and saved sequences for LSTM!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Categorize sentiment\n",
    "http_df = http_df.withColumn(\n",
    "    \"sentiment_category\",\n",
    "    when(col(\"sentiment_score\") > 0.2, \"positive\")\n",
    "    .when(col(\"sentiment_score\") < -0.2, \"negative\")\n",
    "    .otherwise(\"neutral\")\n",
    ")\n",
    "\n",
    "# Append work/off-hours classification\n",
    "http_df = http_df.withColumn(\"activity\", concat(lit(\"http_\"), col(\"sentiment_category\"), lit(\"_\"), col(\"work_hours\")))\n",
    "\n",
    "http_df.select(\"user\", \"date\", \"url\", \"sentiment_score\", \"activity\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import collect_list\n",
    "\n",
    "# Group activities per user-day\n",
    "user_day_sequences = (\n",
    "    logon_df.union(device_df).union(http_df).union(email_df).union(file_df)\n",
    "    .groupBy(\"user\", \"date\")\n",
    "    .agg(collect_list(\"activity\").alias(\"activity_sequence\"))\n",
    ")\n",
    "\n",
    "user_day_sequences.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, hour, when\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder.appName(\"CERT-Preprocessing\").getOrCreate()\n",
    "\n",
    "# Load CSV files\n",
    "logon_df = spark.read.csv(\"processed_logon.csv\", header=True, inferSchema=True)\n",
    "device_df = spark.read.csv(\"processed_device.csv\", header=True, inferSchema=True)\n",
    "http_df = spark.read.csv(\"processed_http.csv\", header=True, inferSchema=True)\n",
    "email_df = spark.read.csv(\"final_email.csv\", header=True, inferSchema=True)\n",
    "file_df = spark.read.csv(\"processed_file.csv\", header=True, inferSchema=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timestamps to user-day format\n",
    "logon_df = logon_df.withColumn(\"date\", to_date(col(\"date\")))\n",
    "device_df = device_df.withColumn(\"date\", to_date(col(\"date\")))\n",
    "http_df = http_df.withColumn(\"date\", to_date(col(\"date\")))\n",
    "email_df = email_df.withColumn(\"date\", to_date(col(\"date\")))\n",
    "file_df = file_df.withColumn(\"date\", to_date(col(\"date\")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `activity` cannot be resolved. Did you mean one of the following? [`cc`, `content`, `id`, `size`, `to`].;\n'Project [id#106, date#200, user#108, pc#109, to#110, cc#111, bcc#112, from#113, size#114, attachments#115, content_x#116, anomalous#117, content_y#118, cleaned_content_x#119, email_sentiment_x#120, content#121, cleaned_content_y#122, email_sentiment_y#123, email_activity#124, hour#530, work_hours#590, (('activity + _) + work_hours#590) AS activity#651]\n+- Project [id#106, date#200, user#108, pc#109, to#110, cc#111, bcc#112, from#113, size#114, attachments#115, content_x#116, anomalous#117, content_y#118, cleaned_content_x#119, email_sentiment_x#120, content#121, cleaned_content_y#122, email_sentiment_y#123, email_activity#124, hour#530, CASE WHEN ((hour#530 >= 8) AND (hour#530 <= 18)) THEN work ELSE offhours END AS work_hours#590]\n   +- Project [id#106, date#200, user#108, pc#109, to#110, cc#111, bcc#112, from#113, size#114, attachments#115, content_x#116, anomalous#117, content_y#118, cleaned_content_x#119, email_sentiment_x#120, content#121, cleaned_content_y#122, email_sentiment_y#123, email_activity#124, hour(cast(date#200 as timestamp), Some(Asia/Calcutta)) AS hour#530, work_hours#452]\n      +- Project [id#106, date#200, user#108, pc#109, to#110, cc#111, bcc#112, from#113, size#114, attachments#115, content_x#116, anomalous#117, content_y#118, cleaned_content_x#119, email_sentiment_x#120, content#121, cleaned_content_y#122, email_sentiment_y#123, email_activity#124, hour#392, CASE WHEN ((hour#392 >= 8) AND (hour#392 <= 18)) THEN work ELSE offhours END AS work_hours#452]\n         +- Project [id#106, date#200, user#108, pc#109, to#110, cc#111, bcc#112, from#113, size#114, attachments#115, content_x#116, anomalous#117, content_y#118, cleaned_content_x#119, email_sentiment_x#120, content#121, cleaned_content_y#122, email_sentiment_y#123, email_activity#124, hour(cast(date#200 as timestamp), Some(Asia/Calcutta)) AS hour#392, work_hours#313]\n            +- Project [id#106, date#200, user#108, pc#109, to#110, cc#111, bcc#112, from#113, size#114, attachments#115, content_x#116, anomalous#117, content_y#118, cleaned_content_x#119, email_sentiment_x#120, content#121, cleaned_content_y#122, email_sentiment_y#123, email_activity#124, hour#255, CASE WHEN ((hour#255 >= 8) AND (hour#255 <= 18)) THEN work ELSE offhours END AS work_hours#313]\n               +- Project [id#106, date#200, user#108, pc#109, to#110, cc#111, bcc#112, from#113, size#114, attachments#115, content_x#116, anomalous#117, content_y#118, cleaned_content_x#119, email_sentiment_x#120, content#121, cleaned_content_y#122, email_sentiment_y#123, email_activity#124, hour(cast(date#200 as timestamp), Some(Asia/Calcutta)) AS hour#255]\n                  +- Project [id#106, to_date(date#107, None, Some(Asia/Calcutta), false) AS date#200, user#108, pc#109, to#110, cc#111, bcc#112, from#113, size#114, attachments#115, content_x#116, anomalous#117, content_y#118, cleaned_content_x#119, email_sentiment_x#120, content#121, cleaned_content_y#122, email_sentiment_y#123, email_activity#124]\n                     +- Relation [id#106,date#107,user#108,pc#109,to#110,cc#111,bcc#112,from#113,size#114,attachments#115,content_x#116,anomalous#117,content_y#118,cleaned_content_x#119,email_sentiment_x#120,content#121,cleaned_content_y#122,email_sentiment_y#123,email_activity#124] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 25\u001b[0m\n\u001b[0;32m     20\u001b[0m device_df \u001b[38;5;241m=\u001b[39m device_df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactivity\u001b[39m\u001b[38;5;124m\"\u001b[39m, col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactivity\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwork_hours\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     21\u001b[0m http_df \u001b[38;5;241m=\u001b[39m http_df\u001b[38;5;241m.\u001b[39mwithColumn(\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactivity\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     23\u001b[0m     concat(lit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp_access_\u001b[39m\u001b[38;5;124m\"\u001b[39m), col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwork_hours\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     24\u001b[0m )\n\u001b[1;32m---> 25\u001b[0m email_df \u001b[38;5;241m=\u001b[39m \u001b[43memail_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mactivity\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mactivity\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwork_hours\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m file_df \u001b[38;5;241m=\u001b[39m file_df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactivity\u001b[39m\u001b[38;5;124m\"\u001b[39m, col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactivity\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwork_hours\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\bkesh\\miniconda3\\envs\\tf\\lib\\site-packages\\pyspark\\sql\\dataframe.py:5176\u001b[0m, in \u001b[0;36mDataFrame.withColumn\u001b[1;34m(self, colName, col)\u001b[0m\n\u001b[0;32m   5171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, Column):\n\u001b[0;32m   5172\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m   5173\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_COLUMN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5174\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m   5175\u001b[0m     )\n\u001b[1;32m-> 5176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[1;32mc:\\Users\\bkesh\\miniconda3\\envs\\tf\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\bkesh\\miniconda3\\envs\\tf\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `activity` cannot be resolved. Did you mean one of the following? [`cc`, `content`, `id`, `size`, `to`].;\n'Project [id#106, date#200, user#108, pc#109, to#110, cc#111, bcc#112, from#113, size#114, attachments#115, content_x#116, anomalous#117, content_y#118, cleaned_content_x#119, email_sentiment_x#120, content#121, cleaned_content_y#122, email_sentiment_y#123, email_activity#124, hour#530, work_hours#590, (('activity + _) + work_hours#590) AS activity#651]\n+- Project [id#106, date#200, user#108, pc#109, to#110, cc#111, bcc#112, from#113, size#114, attachments#115, content_x#116, anomalous#117, content_y#118, cleaned_content_x#119, email_sentiment_x#120, content#121, cleaned_content_y#122, email_sentiment_y#123, email_activity#124, hour#530, CASE WHEN ((hour#530 >= 8) AND (hour#530 <= 18)) THEN work ELSE offhours END AS work_hours#590]\n   +- Project [id#106, date#200, user#108, pc#109, to#110, cc#111, bcc#112, from#113, size#114, attachments#115, content_x#116, anomalous#117, content_y#118, cleaned_content_x#119, email_sentiment_x#120, content#121, cleaned_content_y#122, email_sentiment_y#123, email_activity#124, hour(cast(date#200 as timestamp), Some(Asia/Calcutta)) AS hour#530, work_hours#452]\n      +- Project [id#106, date#200, user#108, pc#109, to#110, cc#111, bcc#112, from#113, size#114, attachments#115, content_x#116, anomalous#117, content_y#118, cleaned_content_x#119, email_sentiment_x#120, content#121, cleaned_content_y#122, email_sentiment_y#123, email_activity#124, hour#392, CASE WHEN ((hour#392 >= 8) AND (hour#392 <= 18)) THEN work ELSE offhours END AS work_hours#452]\n         +- Project [id#106, date#200, user#108, pc#109, to#110, cc#111, bcc#112, from#113, size#114, attachments#115, content_x#116, anomalous#117, content_y#118, cleaned_content_x#119, email_sentiment_x#120, content#121, cleaned_content_y#122, email_sentiment_y#123, email_activity#124, hour(cast(date#200 as timestamp), Some(Asia/Calcutta)) AS hour#392, work_hours#313]\n            +- Project [id#106, date#200, user#108, pc#109, to#110, cc#111, bcc#112, from#113, size#114, attachments#115, content_x#116, anomalous#117, content_y#118, cleaned_content_x#119, email_sentiment_x#120, content#121, cleaned_content_y#122, email_sentiment_y#123, email_activity#124, hour#255, CASE WHEN ((hour#255 >= 8) AND (hour#255 <= 18)) THEN work ELSE offhours END AS work_hours#313]\n               +- Project [id#106, date#200, user#108, pc#109, to#110, cc#111, bcc#112, from#113, size#114, attachments#115, content_x#116, anomalous#117, content_y#118, cleaned_content_x#119, email_sentiment_x#120, content#121, cleaned_content_y#122, email_sentiment_y#123, email_activity#124, hour(cast(date#200 as timestamp), Some(Asia/Calcutta)) AS hour#255]\n                  +- Project [id#106, to_date(date#107, None, Some(Asia/Calcutta), false) AS date#200, user#108, pc#109, to#110, cc#111, bcc#112, from#113, size#114, attachments#115, content_x#116, anomalous#117, content_y#118, cleaned_content_x#119, email_sentiment_x#120, content#121, cleaned_content_y#122, email_sentiment_y#123, email_activity#124]\n                     +- Relation [id#106,date#107,user#108,pc#109,to#110,cc#111,bcc#112,from#113,size#114,attachments#115,content_x#116,anomalous#117,content_y#118,cleaned_content_x#119,email_sentiment_x#120,content#121,cleaned_content_y#122,email_sentiment_y#123,email_activity#124] csv\n"
     ]
    }
   ],
   "source": [
    "# Extract hour for work/off-hours classification\n",
    "logon_df = logon_df.withColumn(\"hour\", hour(col(\"date\")))\n",
    "device_df = device_df.withColumn(\"hour\", hour(col(\"date\")))\n",
    "http_df = http_df.withColumn(\"hour\", hour(col(\"date\")))\n",
    "email_df = email_df.withColumn(\"hour\", hour(col(\"date\")))\n",
    "file_df = file_df.withColumn(\"hour\", hour(col(\"date\")))\n",
    "\n",
    "# Classify activities into work or off-hours\n",
    "logon_df = logon_df.withColumn(\"work_hours\", when((col(\"hour\") >= 8) & (col(\"hour\") <= 18), \"work\").otherwise(\"offhours\"))\n",
    "device_df = device_df.withColumn(\"work_hours\", when((col(\"hour\") >= 8) & (col(\"hour\") <= 18), \"work\").otherwise(\"offhours\"))\n",
    "http_df = http_df.withColumn(\"work_hours\", when((col(\"hour\") >= 8) & (col(\"hour\") <= 18), \"work\").otherwise(\"offhours\"))\n",
    "email_df = email_df.withColumn(\"work_hours\", when((col(\"hour\") >= 8) & (col(\"hour\") <= 18), \"work\").otherwise(\"offhours\"))\n",
    "file_df = file_df.withColumn(\"work_hours\", when((col(\"hour\") >= 8) & (col(\"hour\") <= 18), \"work\").otherwise(\"offhours\"))\n",
    "\n",
    "from pyspark.sql.functions import concat, lit, col\n",
    "\n",
    "\n",
    "# Append work/off-hours classification to activity names\n",
    "logon_df = logon_df.withColumn(\"activity\", col(\"activity\") + \"_\" + col(\"work_hours\"))\n",
    "device_df = device_df.withColumn(\"activity\", col(\"activity\") + \"_\" + col(\"work_hours\"))\n",
    "http_df = http_df.withColumn(\n",
    "    \"activity\",\n",
    "    concat(lit(\"http_access_\"), col(\"work_hours\"))\n",
    ")\n",
    "http_df = http_df.withColumn(\n",
    "    \"activity\",\n",
    "    concat(lit(\"http_access_\"), col(\"work_hours\"))\n",
    ")\n",
    "file_df = file_df.withColumn(\"activity\", col(\"activity\") + \"_\" + col(\"work_hours\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define 24 activity categories (12 work + 12 off-hours)\n",
    "activity_mapping = {\n",
    "    \"logon_work\": 0, \"logoff_work\": 1, \"file_open_work\": 2, \"file_copy_work\": 3,\n",
    "    \"file_delete_work\": 4, \"web_browsing_work\": 5, \"email_sent_work\": 6, \"usb_insert_work\": 7,\n",
    "    \"email_attachment_work\": 8, \"logon_offhours\": 9, \"logoff_offhours\": 10, \"file_open_offhours\": 11,\n",
    "    \"file_copy_offhours\": 12, \"file_delete_offhours\": 13, \"web_browsing_offhours\": 14, \"email_sent_offhours\": 15,\n",
    "    \"usb_insert_offhours\": 16, \"email_attachment_offhours\": 17, \"http_access_work\": 18, \"http_access_offhours\": 19,\n",
    "    \"file_transfer_work\": 20, \"file_transfer_offhours\": 21, \"external_email_work\": 22, \"external_email_offhours\": 23\n",
    "}\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Apply activity encoding\n",
    "activity_mapping_udf = udf(lambda activity: activity_mapping.get(activity, 24), IntegerType())\n",
    "\n",
    "logon_df = logon_df.withColumn(\"activity_encoded\", activity_mapping_udf(col(\"activity\")))\n",
    "device_df = device_df.withColumn(\"activity_encoded\", activity_mapping_udf(col(\"activity\")))\n",
    "http_df = http_df.withColumn(\"activity_encoded\", activity_mapping_udf(col(\"activity\")))\n",
    "email_df = email_df.withColumn(\"activity_encoded\", activity_mapping_udf(col(\"activity\")))\n",
    "file_df = file_df.withColumn(\"activity_encoded\", activity_mapping_udf(col(\"activity\")))\n",
    "\n",
    "# Show transformed data\n",
    "logon_df.select(\"user\", \"date\", \"activity\", \"activity_encoded\").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define work hours\n",
    "WORK_START = 7  # 9 AM\n",
    "WORK_END = 18   # 5 PM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the logon.csv file\n",
    "logon_df = pd.read_csv(\"logon.csv\").drop(columns=['id'])\n",
    "device_df=pd.read_csv(\"device.csv\").drop(columns=['id','pc'])\n",
    "file_df=pd.read_csv(\"file.csv\").drop(columns=['id','content'])\n",
    "http_df = pd.read_csv(\"http.csv\").drop(columns=['id','pc','url','content'])\n",
    "email_df= pd.read_csv(\"email.csv\").drop(columns=['id','pc','cc','bcc','size','content'])\n",
    "\n",
    "# Display the first few rows\n",
    "print(email_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>user</th>\n",
       "      <th>to</th>\n",
       "      <th>from</th>\n",
       "      <th>attachments</th>\n",
       "      <th>anomalous</th>\n",
       "      <th>email_sentiment_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01/02/2010 07:11:45</td>\n",
       "      <td>LAP0338</td>\n",
       "      <td>Dean.Flynn.Hines@dtaa.com;Wade_Harrison@lockhe...</td>\n",
       "      <td>Lynn.Adena.Pratt@dtaa.com</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01/02/2010 07:13:00</td>\n",
       "      <td>LAP0338</td>\n",
       "      <td>Penelope_Colon@netzero.com</td>\n",
       "      <td>Lynn_A_Pratt@earthlink.net</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.191667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01/02/2010 07:13:17</td>\n",
       "      <td>LAP0338</td>\n",
       "      <td>Judith_Hayden@comcast.net</td>\n",
       "      <td>Lynn_A_Pratt@earthlink.net</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01/02/2010 08:10:07</td>\n",
       "      <td>HSB0196</td>\n",
       "      <td>Sawyer.A.Turner@sbcglobal.net</td>\n",
       "      <td>Hadley.S.Bowen@charter.net</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01/02/2010 08:33:56</td>\n",
       "      <td>HSB0196</td>\n",
       "      <td>Sawyer.Abel.Turner@dtaa.com</td>\n",
       "      <td>Hadley.Sonya.Bowen@dtaa.com</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.208333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1115161</th>\n",
       "      <td>05/16/2011 20:46:19</td>\n",
       "      <td>OSH0655</td>\n",
       "      <td>Brewer-Armand@hotmail.com;Kay-Skinner@yahoo.co...</td>\n",
       "      <td>Harrison_Olympia@earthlink.net</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.047500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1115162</th>\n",
       "      <td>05/16/2011 20:49:51</td>\n",
       "      <td>OSH0655</td>\n",
       "      <td>Anjolie.Bowman@gmail.com</td>\n",
       "      <td>Harrison_Olympia@earthlink.net</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.012037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1115163</th>\n",
       "      <td>05/16/2011 20:52:54</td>\n",
       "      <td>DID0650</td>\n",
       "      <td>Talley_Eagan@msn.com;Jonah.M.Wilder@aol.com;Ur...</td>\n",
       "      <td>Denise.I.Doyle@optonline.net</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.053571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1115164</th>\n",
       "      <td>05/16/2011 20:54:43</td>\n",
       "      <td>LAF0991</td>\n",
       "      <td>Hu.Akeem.Vincent@dtaa.com</td>\n",
       "      <td>Lucas.Ahmed.Ferrell@dtaa.com</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1115165</th>\n",
       "      <td>05/16/2011 21:08:12</td>\n",
       "      <td>LAF0991</td>\n",
       "      <td>Doyle.Grant@netzero.com;HAV856@charter.net</td>\n",
       "      <td>Ferrell.Lucas@sbcglobal.net</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.193750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1115166 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        date     user  \\\n",
       "0        01/02/2010 07:11:45  LAP0338   \n",
       "1        01/02/2010 07:13:00  LAP0338   \n",
       "2        01/02/2010 07:13:17  LAP0338   \n",
       "3        01/02/2010 08:10:07  HSB0196   \n",
       "4        01/02/2010 08:33:56  HSB0196   \n",
       "...                      ...      ...   \n",
       "1115161  05/16/2011 20:46:19  OSH0655   \n",
       "1115162  05/16/2011 20:49:51  OSH0655   \n",
       "1115163  05/16/2011 20:52:54  DID0650   \n",
       "1115164  05/16/2011 20:54:43  LAF0991   \n",
       "1115165  05/16/2011 21:08:12  LAF0991   \n",
       "\n",
       "                                                        to  \\\n",
       "0        Dean.Flynn.Hines@dtaa.com;Wade_Harrison@lockhe...   \n",
       "1                               Penelope_Colon@netzero.com   \n",
       "2                                Judith_Hayden@comcast.net   \n",
       "3                            Sawyer.A.Turner@sbcglobal.net   \n",
       "4                              Sawyer.Abel.Turner@dtaa.com   \n",
       "...                                                    ...   \n",
       "1115161  Brewer-Armand@hotmail.com;Kay-Skinner@yahoo.co...   \n",
       "1115162                           Anjolie.Bowman@gmail.com   \n",
       "1115163  Talley_Eagan@msn.com;Jonah.M.Wilder@aol.com;Ur...   \n",
       "1115164                          Hu.Akeem.Vincent@dtaa.com   \n",
       "1115165         Doyle.Grant@netzero.com;HAV856@charter.net   \n",
       "\n",
       "                                   from  attachments  anomalous  \\\n",
       "0             Lynn.Adena.Pratt@dtaa.com            0          0   \n",
       "1            Lynn_A_Pratt@earthlink.net            0          0   \n",
       "2            Lynn_A_Pratt@earthlink.net            0          0   \n",
       "3            Hadley.S.Bowen@charter.net            0          0   \n",
       "4           Hadley.Sonya.Bowen@dtaa.com            3          0   \n",
       "...                                 ...          ...        ...   \n",
       "1115161  Harrison_Olympia@earthlink.net            2          0   \n",
       "1115162  Harrison_Olympia@earthlink.net            0          0   \n",
       "1115163    Denise.I.Doyle@optonline.net            0          0   \n",
       "1115164    Lucas.Ahmed.Ferrell@dtaa.com            1          0   \n",
       "1115165     Ferrell.Lucas@sbcglobal.net            1          0   \n",
       "\n",
       "         email_sentiment_y  \n",
       "0                 0.100000  \n",
       "1                -0.191667  \n",
       "2                 0.050000  \n",
       "3                 0.000000  \n",
       "4                -0.208333  \n",
       "...                    ...  \n",
       "1115161          -0.047500  \n",
       "1115162           0.012037  \n",
       "1115163           0.053571  \n",
       "1115164           0.000000  \n",
       "1115165           0.193750  \n",
       "\n",
       "[1115166 rows x 7 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email_df.drop(columns=['content_x','content_y','cleaned_content_x','cleaned_content_y','email_activity','email_sentiment_x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_df=pd.read_csv(\"device.csv\").drop(columns=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_mapping = {\n",
    "    # ✅ Logon & Logoff\n",
    "    \"logon_work\": 0, \"logoff_work\": 1,\n",
    "    \"logon_offhours\": 15, \"logoff_offhours\": 16,\n",
    "\n",
    "    # ✅ File Activities\n",
    "    \"file_copy_work\": 2, \"file_copy_offhours\": 17, \n",
    "\n",
    "    # ✅ Web Browsing / HTTP\n",
    "   \n",
    "    \n",
    "\n",
    "    # ✅ Email Activities\n",
    "    \"email_sent_work\": 3, \"email_sent_offhours\": 18,\n",
    "    \"email_received_work\": 4, \"email_received_offhours\": 29,\n",
    "    \"email_attachment_work\": 5, \"email_attachment_offhours\": 20,\n",
    "    \"email_positive_work\": 6, \"email_positive_offhours\": 21,\n",
    "    \"email_negative_work\": 7, \"email_negative_offhours\": 22,\n",
    "    \"email_neutral_work\": 8, \"email_neutral_offhours\": 23,\n",
    "\n",
    "    # ✅ USB Device (Thumb Drive)\n",
    "    \"thumbdrive_connected_work\": 9, \"thumbdrive_connected_offhours\": 24,\n",
    "    \"thumbdrive_disconnected_work\": 10, \"thumbdrive_disconnected_offhours\": 25,\n",
    "\n",
    "    # ✅ File Transfers\n",
    "    \n",
    "    \n",
    "    \"http_request_work\": 11, \"http_request_offhours\": 26,\n",
    "    \"http_positive_work\": 12, \"http_positive_offhours\": 27,\n",
    "    \"http_negative_work\": 13, \"http_negative_offhours\": 28,\n",
    "    \"http_neutral_work\": 14, \"http_neutral_offhours\": 29\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'date' column to datetime\n",
    "logon_df['date'] = pd.to_datetime(logon_df['date'])\n",
    "device_df['date'] = pd.to_datetime(device_df['date'])\n",
    "email_df['date'] = pd.to_datetime(email_df['date'])\n",
    "http_df['date'] = pd.to_datetime(http_df['date'])\n",
    "file_df['date'] = pd.to_datetime(file_df['date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_logon_event(row):\n",
    "    hour = row['date'].hour  # Extract the hour from timestamp\n",
    "\n",
    "    if row['activity'] == 'logon':\n",
    "        return 'logon_work' if WORK_START <= hour < WORK_END else 'logon_offhours'\n",
    "    elif row['activity'] == 'logoff':\n",
    "        return 'logoff_work' if WORK_START <= hour < WORK_END else 'logoff_offhours'\n",
    "    return None  # Ignore other cases\n",
    "\n",
    "def classify_device_event(row):\n",
    "    \"\"\"Classifies device connection/disconnection based on work hours.\"\"\"\n",
    "    hour = pd.to_datetime(row['date']).hour  # Extract hour from timestamp\n",
    "    \n",
    "    if pd.notna(row['pc']):  # Only process rows where 'pc' is not empty\n",
    "        if row['activity'] == 'connect':\n",
    "            return 'thumbdrive_connected_work' if WORK_START <= hour < WORK_END else 'thumbdrive_connected_offhours'\n",
    "        elif row['activity'] == 'disconnect':\n",
    "            return 'thumbdrive_disconnected_work' if WORK_START <= hour < WORK_END else 'thumbdrive_disconnected_offhours'\n",
    "    \n",
    "    return None  # Ignore cases where 'pc' is empty\n",
    "\n",
    "def classify_email_activity(row):\n",
    "    \"\"\"Assigns multiple activity types to each email event.\"\"\"\n",
    "    hour = pd.to_datetime(row['date']).hour  # Extract hour\n",
    "\n",
    "    # Determine if work or off-hours\n",
    "    is_work_hours = WORK_START <= hour < WORK_END\n",
    "\n",
    "    activity_types = []  # Store multiple labels\n",
    "\n",
    "    # Email Sent\n",
    "    if row[\"from\"]:  # If there is a sender\n",
    "        activity_types.append(\"email_sent_work\" if is_work_hours else \"email_sent_offhours\")\n",
    "\n",
    "    # Email Received\n",
    "    if row[\"to\"]:  # If there are recipients\n",
    "        activity_types.append(\"email_received_work\" if is_work_hours else \"email_received_offhours\")\n",
    "\n",
    "    # Email with Attachment\n",
    "    if row[\"attachments\"] > 0:\n",
    "        activity_types.append(\"email_attachment_work\" if is_work_hours else \"email_attachment_offhours\")\n",
    "\n",
    "    # Email Sentiment Classification\n",
    "    sentiment = row[\"email_sentiment_y\"]\n",
    "    if sentiment > 0.2:\n",
    "        activity_types.append(\"email_positive_work\" if is_work_hours else \"email_positive_offhours\")\n",
    "    elif sentiment < -0.2:\n",
    "        activity_types.append(\"email_negative_work\" if is_work_hours else \"email_negative_offhours\")\n",
    "    else:\n",
    "        activity_types.append(\"email_neutral_work\" if is_work_hours else \"email_neutral_offhours\")\n",
    "\n",
    "    # Convert activity types to encoded values\n",
    "    return [activity_mapping[act] for act in activity_types]\n",
    "\n",
    "def classify_file_activity(row):\n",
    "    \"\"\"Encodes file copy activity based on work hours.\"\"\"\n",
    "    hour = pd.to_datetime(row['date']).hour  # Extract hour\n",
    "\n",
    "    # Check if both 'pc' and 'filename' are not empty\n",
    "    if pd.notna(row['pc']) and pd.notna(row['filename']):\n",
    "        return activity_mapping[\"file_copy_work\"] if WORK_START <= hour < WORK_END else activity_mapping[\"file_copy_offhours\"]\n",
    "    \n",
    "    return None  # Ignore events where pc or filename is missing\n",
    "\n",
    "def encode_http_activity(row):\n",
    "    \"\"\"Encodes HTTP request activity based on work hours and sentiment.\"\"\"\n",
    "    hour = pd.to_datetime(row['date']).hour  # Extract hour from timestamp\n",
    "    is_work_hours = WORK_START <= hour < WORK_END\n",
    "\n",
    "    activity_types = []  # Store multiple activity labels\n",
    "\n",
    "    # General HTTP request activity\n",
    "    activity_types.append(\"http_request_work\" if is_work_hours else \"http_request_offhours\")\n",
    "\n",
    "    # Browsing Sentiment Classification\n",
    "    sentiment = row[\"sentiment\"]\n",
    "    if sentiment > 0.2:\n",
    "        activity_types.append(\"http_positive_work\" if is_work_hours else \"http_positive_offhours\")\n",
    "    elif sentiment < -0.2:\n",
    "        activity_types.append(\"http_negative_work\" if is_work_hours else \"http_negative_offhours\")\n",
    "    else:\n",
    "        activity_types.append(\"http_neutral_work\" if is_work_hours else \"http_neutral_offhours\")\n",
    "\n",
    "    # Convert activity types to encoded values\n",
    "    return [activity_mapping[act] for act in activity_types]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function\n",
    "logon_df['activity_type'] = logon_df.apply(classify_logon_event, axis=1)\n",
    "device_df['activity_type'] = device_df.apply(classify_device_event, axis=1)\n",
    "email_df['activity_sequence'] = email_df.apply(classify_email_activity, axis=1)\n",
    "file_df['activity_type'] = file_df.apply(classify_file_activity, axis=1)\n",
    "http_df['activity_sequence'] = http_df.apply(encode_http_activity, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert activity types to numeric values\n",
    "logon_df['encoded_activity'] = logon_df['activity_type'].map(activity_mapping)\n",
    "device_df['encoded_activity'] = device_df['activity_type'].map(activity_mapping)\n",
    "file_df['encoded_activity'] = file_df['activity_type'].map(activity_mapping)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>user</th>\n",
       "      <th>to</th>\n",
       "      <th>from</th>\n",
       "      <th>attachments</th>\n",
       "      <th>content_x</th>\n",
       "      <th>anomalous</th>\n",
       "      <th>content_y</th>\n",
       "      <th>cleaned_content_x</th>\n",
       "      <th>email_sentiment_x</th>\n",
       "      <th>cleaned_content_y</th>\n",
       "      <th>email_sentiment_y</th>\n",
       "      <th>email_activity</th>\n",
       "      <th>activity_sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-02 07:11:45</td>\n",
       "      <td>LAP0338</td>\n",
       "      <td>Dean.Flynn.Hines@dtaa.com;Wade_Harrison@lockhe...</td>\n",
       "      <td>Lynn.Adena.Pratt@dtaa.com</td>\n",
       "      <td>0</td>\n",
       "      <td>middle f2 systems 4 july techniques powerful d...</td>\n",
       "      <td>0</td>\n",
       "      <td>middle f2 systems 4 july techniques powerful d...</td>\n",
       "      <td>middle f2 systems 4 july techniques powerful d...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>middle f2 systems 4 july techniques powerful d...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>email_neutral</td>\n",
       "      <td>[18, 29, 23]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-02 07:13:00</td>\n",
       "      <td>LAP0338</td>\n",
       "      <td>Penelope_Colon@netzero.com</td>\n",
       "      <td>Lynn_A_Pratt@earthlink.net</td>\n",
       "      <td>0</td>\n",
       "      <td>slowly this uncinus winter beneath addition ex...</td>\n",
       "      <td>0</td>\n",
       "      <td>slowly this uncinus winter beneath addition ex...</td>\n",
       "      <td>slowly uncinus winter beneath addition exist p...</td>\n",
       "      <td>-0.191667</td>\n",
       "      <td>slowly uncinus winter beneath addition exist p...</td>\n",
       "      <td>-0.191667</td>\n",
       "      <td>email_neutral</td>\n",
       "      <td>[18, 29, 23]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-01-02 07:13:17</td>\n",
       "      <td>LAP0338</td>\n",
       "      <td>Judith_Hayden@comcast.net</td>\n",
       "      <td>Lynn_A_Pratt@earthlink.net</td>\n",
       "      <td>0</td>\n",
       "      <td>400 other difficult land cirrocumulus powered ...</td>\n",
       "      <td>0</td>\n",
       "      <td>400 other difficult land cirrocumulus powered ...</td>\n",
       "      <td>400 difficult land cirrocumulus powered probab...</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>400 difficult land cirrocumulus powered probab...</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>email_neutral</td>\n",
       "      <td>[18, 29, 23]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-01-02 08:10:07</td>\n",
       "      <td>HSB0196</td>\n",
       "      <td>Sawyer.A.Turner@sbcglobal.net</td>\n",
       "      <td>Hadley.S.Bowen@charter.net</td>\n",
       "      <td>0</td>\n",
       "      <td>enacted saw 63 recently 5 were representatives...</td>\n",
       "      <td>0</td>\n",
       "      <td>enacted saw 63 recently 5 were representatives...</td>\n",
       "      <td>enacted saw 63 recently 5 representatives safe...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>enacted saw 63 recently 5 representatives safe...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>email_neutral</td>\n",
       "      <td>[18, 29, 23]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-01-02 08:33:56</td>\n",
       "      <td>HSB0196</td>\n",
       "      <td>Sawyer.Abel.Turner@dtaa.com</td>\n",
       "      <td>Hadley.Sonya.Bowen@dtaa.com</td>\n",
       "      <td>3</td>\n",
       "      <td>attitude basis you collapse note found words r...</td>\n",
       "      <td>0</td>\n",
       "      <td>attitude basis you collapse note found words r...</td>\n",
       "      <td>attitude basis collapse note found words range...</td>\n",
       "      <td>-0.208333</td>\n",
       "      <td>attitude basis collapse note found words range...</td>\n",
       "      <td>-0.208333</td>\n",
       "      <td>email_negative</td>\n",
       "      <td>[18, 29, 20, 22]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 date     user  \\\n",
       "0 2010-01-02 07:11:45  LAP0338   \n",
       "1 2010-01-02 07:13:00  LAP0338   \n",
       "2 2010-01-02 07:13:17  LAP0338   \n",
       "3 2010-01-02 08:10:07  HSB0196   \n",
       "4 2010-01-02 08:33:56  HSB0196   \n",
       "\n",
       "                                                  to  \\\n",
       "0  Dean.Flynn.Hines@dtaa.com;Wade_Harrison@lockhe...   \n",
       "1                         Penelope_Colon@netzero.com   \n",
       "2                          Judith_Hayden@comcast.net   \n",
       "3                      Sawyer.A.Turner@sbcglobal.net   \n",
       "4                        Sawyer.Abel.Turner@dtaa.com   \n",
       "\n",
       "                          from  attachments  \\\n",
       "0    Lynn.Adena.Pratt@dtaa.com            0   \n",
       "1   Lynn_A_Pratt@earthlink.net            0   \n",
       "2   Lynn_A_Pratt@earthlink.net            0   \n",
       "3   Hadley.S.Bowen@charter.net            0   \n",
       "4  Hadley.Sonya.Bowen@dtaa.com            3   \n",
       "\n",
       "                                           content_x  anomalous  \\\n",
       "0  middle f2 systems 4 july techniques powerful d...          0   \n",
       "1  slowly this uncinus winter beneath addition ex...          0   \n",
       "2  400 other difficult land cirrocumulus powered ...          0   \n",
       "3  enacted saw 63 recently 5 were representatives...          0   \n",
       "4  attitude basis you collapse note found words r...          0   \n",
       "\n",
       "                                           content_y  \\\n",
       "0  middle f2 systems 4 july techniques powerful d...   \n",
       "1  slowly this uncinus winter beneath addition ex...   \n",
       "2  400 other difficult land cirrocumulus powered ...   \n",
       "3  enacted saw 63 recently 5 were representatives...   \n",
       "4  attitude basis you collapse note found words r...   \n",
       "\n",
       "                                   cleaned_content_x  email_sentiment_x  \\\n",
       "0  middle f2 systems 4 july techniques powerful d...           0.100000   \n",
       "1  slowly uncinus winter beneath addition exist p...          -0.191667   \n",
       "2  400 difficult land cirrocumulus powered probab...           0.050000   \n",
       "3  enacted saw 63 recently 5 representatives safe...           0.000000   \n",
       "4  attitude basis collapse note found words range...          -0.208333   \n",
       "\n",
       "                                   cleaned_content_y  email_sentiment_y  \\\n",
       "0  middle f2 systems 4 july techniques powerful d...           0.100000   \n",
       "1  slowly uncinus winter beneath addition exist p...          -0.191667   \n",
       "2  400 difficult land cirrocumulus powered probab...           0.050000   \n",
       "3  enacted saw 63 recently 5 representatives safe...           0.000000   \n",
       "4  attitude basis collapse note found words range...          -0.208333   \n",
       "\n",
       "   email_activity activity_sequence  \n",
       "0   email_neutral      [18, 29, 23]  \n",
       "1   email_neutral      [18, 29, 23]  \n",
       "2   email_neutral      [18, 29, 23]  \n",
       "3   email_neutral      [18, 29, 23]  \n",
       "4  email_negative  [18, 29, 20, 22]  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Expand sequences (convert list from string format)\n",
    "# email_df['activity_sequence'] = email_df['activity_sequence'].apply(eval)\n",
    "# http_df['activity_sequence'] = http_df['activity_sequence'].apply(eval)\n",
    "\n",
    "# Explode email & http sequences (each activity gets its own row)\n",
    "email_df = email_df.explode('activity_sequence').rename(columns={'activity_sequence': 'activity_encoded'})\n",
    "http_df = http_df.explode('activity_sequence').rename(columns={'activity_sequence': 'activity_encoded'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>user</th>\n",
       "      <th>to</th>\n",
       "      <th>from</th>\n",
       "      <th>attachments</th>\n",
       "      <th>content_x</th>\n",
       "      <th>anomalous</th>\n",
       "      <th>content_y</th>\n",
       "      <th>cleaned_content_x</th>\n",
       "      <th>email_sentiment_x</th>\n",
       "      <th>cleaned_content_y</th>\n",
       "      <th>email_sentiment_y</th>\n",
       "      <th>email_activity</th>\n",
       "      <th>activity_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-02 07:11:45</td>\n",
       "      <td>LAP0338</td>\n",
       "      <td>Dean.Flynn.Hines@dtaa.com;Wade_Harrison@lockhe...</td>\n",
       "      <td>Lynn.Adena.Pratt@dtaa.com</td>\n",
       "      <td>0</td>\n",
       "      <td>middle f2 systems 4 july techniques powerful d...</td>\n",
       "      <td>0</td>\n",
       "      <td>middle f2 systems 4 july techniques powerful d...</td>\n",
       "      <td>middle f2 systems 4 july techniques powerful d...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>middle f2 systems 4 july techniques powerful d...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>email_neutral</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-02 07:11:45</td>\n",
       "      <td>LAP0338</td>\n",
       "      <td>Dean.Flynn.Hines@dtaa.com;Wade_Harrison@lockhe...</td>\n",
       "      <td>Lynn.Adena.Pratt@dtaa.com</td>\n",
       "      <td>0</td>\n",
       "      <td>middle f2 systems 4 july techniques powerful d...</td>\n",
       "      <td>0</td>\n",
       "      <td>middle f2 systems 4 july techniques powerful d...</td>\n",
       "      <td>middle f2 systems 4 july techniques powerful d...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>middle f2 systems 4 july techniques powerful d...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>email_neutral</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-02 07:11:45</td>\n",
       "      <td>LAP0338</td>\n",
       "      <td>Dean.Flynn.Hines@dtaa.com;Wade_Harrison@lockhe...</td>\n",
       "      <td>Lynn.Adena.Pratt@dtaa.com</td>\n",
       "      <td>0</td>\n",
       "      <td>middle f2 systems 4 july techniques powerful d...</td>\n",
       "      <td>0</td>\n",
       "      <td>middle f2 systems 4 july techniques powerful d...</td>\n",
       "      <td>middle f2 systems 4 july techniques powerful d...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>middle f2 systems 4 july techniques powerful d...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>email_neutral</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-02 07:13:00</td>\n",
       "      <td>LAP0338</td>\n",
       "      <td>Penelope_Colon@netzero.com</td>\n",
       "      <td>Lynn_A_Pratt@earthlink.net</td>\n",
       "      <td>0</td>\n",
       "      <td>slowly this uncinus winter beneath addition ex...</td>\n",
       "      <td>0</td>\n",
       "      <td>slowly this uncinus winter beneath addition ex...</td>\n",
       "      <td>slowly uncinus winter beneath addition exist p...</td>\n",
       "      <td>-0.191667</td>\n",
       "      <td>slowly uncinus winter beneath addition exist p...</td>\n",
       "      <td>-0.191667</td>\n",
       "      <td>email_neutral</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-02 07:13:00</td>\n",
       "      <td>LAP0338</td>\n",
       "      <td>Penelope_Colon@netzero.com</td>\n",
       "      <td>Lynn_A_Pratt@earthlink.net</td>\n",
       "      <td>0</td>\n",
       "      <td>slowly this uncinus winter beneath addition ex...</td>\n",
       "      <td>0</td>\n",
       "      <td>slowly this uncinus winter beneath addition ex...</td>\n",
       "      <td>slowly uncinus winter beneath addition exist p...</td>\n",
       "      <td>-0.191667</td>\n",
       "      <td>slowly uncinus winter beneath addition exist p...</td>\n",
       "      <td>-0.191667</td>\n",
       "      <td>email_neutral</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 date     user  \\\n",
       "0 2010-01-02 07:11:45  LAP0338   \n",
       "0 2010-01-02 07:11:45  LAP0338   \n",
       "0 2010-01-02 07:11:45  LAP0338   \n",
       "1 2010-01-02 07:13:00  LAP0338   \n",
       "1 2010-01-02 07:13:00  LAP0338   \n",
       "\n",
       "                                                  to  \\\n",
       "0  Dean.Flynn.Hines@dtaa.com;Wade_Harrison@lockhe...   \n",
       "0  Dean.Flynn.Hines@dtaa.com;Wade_Harrison@lockhe...   \n",
       "0  Dean.Flynn.Hines@dtaa.com;Wade_Harrison@lockhe...   \n",
       "1                         Penelope_Colon@netzero.com   \n",
       "1                         Penelope_Colon@netzero.com   \n",
       "\n",
       "                         from  attachments  \\\n",
       "0   Lynn.Adena.Pratt@dtaa.com            0   \n",
       "0   Lynn.Adena.Pratt@dtaa.com            0   \n",
       "0   Lynn.Adena.Pratt@dtaa.com            0   \n",
       "1  Lynn_A_Pratt@earthlink.net            0   \n",
       "1  Lynn_A_Pratt@earthlink.net            0   \n",
       "\n",
       "                                           content_x  anomalous  \\\n",
       "0  middle f2 systems 4 july techniques powerful d...          0   \n",
       "0  middle f2 systems 4 july techniques powerful d...          0   \n",
       "0  middle f2 systems 4 july techniques powerful d...          0   \n",
       "1  slowly this uncinus winter beneath addition ex...          0   \n",
       "1  slowly this uncinus winter beneath addition ex...          0   \n",
       "\n",
       "                                           content_y  \\\n",
       "0  middle f2 systems 4 july techniques powerful d...   \n",
       "0  middle f2 systems 4 july techniques powerful d...   \n",
       "0  middle f2 systems 4 july techniques powerful d...   \n",
       "1  slowly this uncinus winter beneath addition ex...   \n",
       "1  slowly this uncinus winter beneath addition ex...   \n",
       "\n",
       "                                   cleaned_content_x  email_sentiment_x  \\\n",
       "0  middle f2 systems 4 july techniques powerful d...           0.100000   \n",
       "0  middle f2 systems 4 july techniques powerful d...           0.100000   \n",
       "0  middle f2 systems 4 july techniques powerful d...           0.100000   \n",
       "1  slowly uncinus winter beneath addition exist p...          -0.191667   \n",
       "1  slowly uncinus winter beneath addition exist p...          -0.191667   \n",
       "\n",
       "                                   cleaned_content_y  email_sentiment_y  \\\n",
       "0  middle f2 systems 4 july techniques powerful d...           0.100000   \n",
       "0  middle f2 systems 4 july techniques powerful d...           0.100000   \n",
       "0  middle f2 systems 4 july techniques powerful d...           0.100000   \n",
       "1  slowly uncinus winter beneath addition exist p...          -0.191667   \n",
       "1  slowly uncinus winter beneath addition exist p...          -0.191667   \n",
       "\n",
       "  email_activity activity_encoded  \n",
       "0  email_neutral               18  \n",
       "0  email_neutral               29  \n",
       "0  email_neutral               23  \n",
       "1  email_neutral               18  \n",
       "1  email_neutral               29  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [logon_df, email_df, http_df, file_df, device_df]:\n",
    "    df['date'] = pd.to_datetime(df['date']).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bkesh\\AppData\\Local\\Temp\\ipykernel_20116\\2168181510.py:2: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([logon_df, email_df, http_df, file_df, device_df])\n"
     ]
    }
   ],
   "source": [
    "# Merge all activities into a single DataFrame\n",
    "merged_df = pd.concat([logon_df, email_df, http_df, file_df, device_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>user</th>\n",
       "      <th>pc</th>\n",
       "      <th>activity</th>\n",
       "      <th>anomalous</th>\n",
       "      <th>activity_type</th>\n",
       "      <th>encoded_activity</th>\n",
       "      <th>to</th>\n",
       "      <th>from</th>\n",
       "      <th>attachments</th>\n",
       "      <th>content_x</th>\n",
       "      <th>content_y</th>\n",
       "      <th>cleaned_content_x</th>\n",
       "      <th>email_sentiment_x</th>\n",
       "      <th>cleaned_content_y</th>\n",
       "      <th>email_sentiment_y</th>\n",
       "      <th>email_activity</th>\n",
       "      <th>activity_encoded</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-02</td>\n",
       "      <td>IRM0931</td>\n",
       "      <td>PC-7188</td>\n",
       "      <td>Logon</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-02</td>\n",
       "      <td>LAP0338</td>\n",
       "      <td>PC-5758</td>\n",
       "      <td>Logon</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-01-02</td>\n",
       "      <td>NOB0181</td>\n",
       "      <td>PC-3446</td>\n",
       "      <td>Logon</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-01-02</td>\n",
       "      <td>CTR0341</td>\n",
       "      <td>PC-6184</td>\n",
       "      <td>Logon</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-01-02</td>\n",
       "      <td>ATE0869</td>\n",
       "      <td>PC-1313</td>\n",
       "      <td>Logon</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date     user       pc activity  anomalous  activity_type  \\\n",
       "0  2010-01-02  IRM0931  PC-7188    Logon          0            NaN   \n",
       "1  2010-01-02  LAP0338  PC-5758    Logon          0            NaN   \n",
       "2  2010-01-02  NOB0181  PC-3446    Logon          0            NaN   \n",
       "3  2010-01-02  CTR0341  PC-6184    Logon          0            NaN   \n",
       "4  2010-01-02  ATE0869  PC-1313    Logon          0            NaN   \n",
       "\n",
       "   encoded_activity   to from  attachments content_x content_y  \\\n",
       "0               NaN  NaN  NaN          NaN       NaN       NaN   \n",
       "1               NaN  NaN  NaN          NaN       NaN       NaN   \n",
       "2               NaN  NaN  NaN          NaN       NaN       NaN   \n",
       "3               NaN  NaN  NaN          NaN       NaN       NaN   \n",
       "4               NaN  NaN  NaN          NaN       NaN       NaN   \n",
       "\n",
       "  cleaned_content_x  email_sentiment_x cleaned_content_y  email_sentiment_y  \\\n",
       "0               NaN                NaN               NaN                NaN   \n",
       "1               NaN                NaN               NaN                NaN   \n",
       "2               NaN                NaN               NaN                NaN   \n",
       "3               NaN                NaN               NaN                NaN   \n",
       "4               NaN                NaN               NaN                NaN   \n",
       "\n",
       "  email_activity activity_encoded  sentiment filename  \n",
       "0            NaN              NaN        NaN      NaN  \n",
       "1            NaN              NaN        NaN      NaN  \n",
       "2            NaN              NaN        NaN      NaN  \n",
       "3            NaN              NaN        NaN      NaN  \n",
       "4            NaN              NaN        NaN      NaN  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_labels = merged_df.groupby(['user', 'date'])['anomalous'].max().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_sequences = merged_df.groupby(['user', 'date'])['activity_encoded'].apply(list).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge anomaly labels with user-day sequences\n",
    "user_sequences = user_sequences.merge(daily_labels, on=['user', 'date'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>date</th>\n",
       "      <th>activity_encoded</th>\n",
       "      <th>anomalous</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAF0535</td>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>[nan, nan, 3, 4, 5, 8, 3, 4, 8, 3, 4, 5, 8, 11...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAF0535</td>\n",
       "      <td>2010-01-05</td>\n",
       "      <td>[nan, nan, 3, 4, 6, 3, 4, 5, 8, 3, 4, 8, 11, 1...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAF0535</td>\n",
       "      <td>2010-01-06</td>\n",
       "      <td>[nan, nan, 3, 4, 6, 3, 4, 7, 11, 14, 11, 12, 1...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAF0535</td>\n",
       "      <td>2010-01-07</td>\n",
       "      <td>[nan, nan, 3, 4, 5, 8, 3, 4, 6, 3, 4, 5, 8, 11...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAF0535</td>\n",
       "      <td>2010-01-08</td>\n",
       "      <td>[nan, nan, 3, 4, 8, 3, 4, 6, 3, 4, 5, 8, 11, 1...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user        date                                   activity_encoded  \\\n",
       "0  AAF0535  2010-01-04  [nan, nan, 3, 4, 5, 8, 3, 4, 8, 3, 4, 5, 8, 11...   \n",
       "1  AAF0535  2010-01-05  [nan, nan, 3, 4, 6, 3, 4, 5, 8, 3, 4, 8, 11, 1...   \n",
       "2  AAF0535  2010-01-06  [nan, nan, 3, 4, 6, 3, 4, 7, 11, 14, 11, 12, 1...   \n",
       "3  AAF0535  2010-01-07  [nan, nan, 3, 4, 5, 8, 3, 4, 6, 3, 4, 5, 8, 11...   \n",
       "4  AAF0535  2010-01-08  [nan, nan, 3, 4, 8, 3, 4, 6, 3, 4, 5, 8, 11, 1...   \n",
       "\n",
       "   anomalous  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  \n",
       "3          0  \n",
       "4          0  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_sequences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast  # To safely parse string lists\n",
    "\n",
    "def process_activity_sequence(seq):\n",
    "    \"\"\"Convert string sequences to lists & replace only NaNs inside the sequence.\"\"\"\n",
    "    \n",
    "    # Convert string representation of lists to actual lists\n",
    "    if isinstance(seq, str):\n",
    "        try:\n",
    "            seq = ast.literal_eval(seq)  # Safer than eval(), converts \"[6, 8, nan]\" → [6, 8, nan]\n",
    "        except:\n",
    "            return [99]  # Fallback if parsing fails\n",
    "    \n",
    "    # If seq is a single NaN, return [99]\n",
    "    if seq is None or (isinstance(seq, float) and np.isnan(seq)):\n",
    "        return [99]\n",
    "\n",
    "    # Ensure it's a list and replace NaNs inside the list\n",
    "    if isinstance(seq, list):\n",
    "        return [99 if (pd.isna(x) or x is None or (isinstance(x, float) and np.isnan(x))) else x for x in seq]\n",
    "    \n",
    "    # If it's still not a list, return [99] as a fallback\n",
    "    return [99]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'activity_sequence' column from string to list & fix NaNs\n",
    "user_sequences['activity_encoded'] = user_sequences['activity_encoded'].apply(process_activity_sequence)\n",
    "\n",
    "# # Ensure all sequences are valid lists\n",
    "# user_sequences = user_sequences.dropna(subset=['activity_encoded'])\n",
    "\n",
    "# Save cleaned dataset\n",
    "user_sequences.to_csv(\"lstm_input_cleaned3.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
